Dataproc is a managed Spark and Hadoop service that lets you take advantage of open source data tools for 
	batch processing, 
	querying, 
	streaming, 
	machine learning. 

Dataproc automation helps you to 
	create clusters quickly, 
	manage them easily, 
	save money by turning clusters off when you don't need them. 


Dataproc clusters can include preemptible instances that have lower compute prices, reducing your costs even further. 

Without using Dataproc, it can take from five to 30 minutes to create Spark and Hadoop clusters on-premises or through IaaS providers. 

Dataproc has built-in integration with other Google Cloud Platform services, such as BigQuery, Cloud Storage, Cloud Bigtable.

When you're done with a cluster, you can simply turn it off, so you don’t spend money on an idle cluster.

You can create a Dataproc cluster 
	via a Dataproc API clusters.create HTTP 
	programmatic request, using the Cloud SDK gcloud command-line tool in 
		a local terminal window 
		or in Cloud Shell, 
		or from the Google Cloud Console opened in a local browser. 
	You can also create clusters programmatically using Cloud Client Libraries, or via API requests.

Compute Engine Virtual Machine instances (VMs) in a Dataproc cluster, consisting of master and worker VMs, 
require full internal IP networking access to each other. 
The <default network> available (and normally used) to create a cluster helps ensure this access.

The generate_custom_image.py program launches a temporary Compute Engine VM instance with the specified Dataproc base image, then runs the customization script inside the VM instance to install custom packages and/or update configurations. After the customization script finishes, it shuts down the VM instance and creates a Cloud Dataproc custom image from the disk of the VM instance. The temporary VM is deleted after the custom image is created. The custom image is saved and can be used to create Dataproc clusters.



You specify the custom image when you create a Dataproc cluster. 
A custom image is saved in Cloud Compute Images, and is valid to create a Cloud Dataproc cluster for 60 days from its creation date

You can create a Dataproc cluster with a custom image using the dataproc clusters create command with the --image flag



You can apply user labels to Dataproc cluster and job resources in order to group resources and related operations for later filtering and listing. You associate labels with resources when the resource is created, at cluster creation or job submission. 

Once a resource is associated with a label, the label is propagated to operations performed on the resource—cluster create, update, patch, or delete; job submit, update, cancel, or delete—allowing you to filter and list clusters, jobs, and operations by label.

Each resource can have multiple labels, up to a maximum of 64.
Each label must be a key-value pair.



You can update a cluster by 
	issuing a Dataproc API clusters.patch request, 


The following cluster parameters can be updated:

    the number of standard worker nodes in a cluster

    the number of preemptible worker nodes in a cluster

    whether to use graceful decommissioning to control shutting down a worker after its jobs are completed

    adding or deleting cluster labels


You can delete a cluster 
	via a Dataproc API clusters.delete HTTP 



Dataproc job logs in Stackdriver

When you run a Dataproc job, job driver output is streamed to the Cloud Console, displayed in the command terminal window (for jobs submitted from the command line), and stored in Cloud Storage (see Accessing job driver output). 

To enable job driver logs in Logging, set the following cluster property when creating the cluster:

    dataproc:dataproc.logging.stackdriver.job.driver.enable=true

Use Stackdriver cluster metrics to monitor the performance and health of Dataproc clusters.

    See Stackdriver Pricing to understand your costs.

    See Monitoring Quotas and limits for information on metric data retention.



Plan to migrate incrementally so you can leave time to migrate jobs,experiment, and test after moving each body of data.

There are two different migration models you should consider for transferring HDFS data to the cloud: 
	push and pull. 


Both models use Hadoop DistCp to copy data from your on-premises HDFS clusters to Cloud Storage, but they use different approaches.

DistCp (distributed copy) is a tool used for large inter/intra-cluster copying.

+ PUSH

The push model is the simplest model: the source cluster runs the distcp jobs on its data nodes and pushes files directly to Cloud Storage, 

+ PULL

The pull model is more complex, but has some advantages. An ephemeral Dataproc cluster runs the distcp jobs on its data nodes, pulls files from the source cluster, and copies them to Cloud Storage.


The push model is the simplest model because the source cluster can push data directly to Cloud Storage and you don't need to create extra compute resources to perform the copy.

If the source cluster is already running at compute capacity, and if you cannot increase the resources on the source cluster to perform the copy, then you should consider using the pull model instead.


Impact on the source cluster's CPU and RAM resources is minimized, because the source nodes are used only for serving blocks out of the cluster. You can also fine-tune the specifications of the pull cluster's resources on Google Cloud to handle the copy jobs, and tear down the pull cluster when the migration is complete.


To understand the implications for network usage for both models, consider how Hadoop handles data replication in HDFS. Hadoop splits each file into multiple blocks — the block size is usually 128-256 megabytes. Hadoop replicates those blocks across multiple data nodes and across multiple racks to avoid losing data in the event of a data node failure or a rack failure. 

The standard configuration is to store 3 replicas of each block.



Plan on moving your data in discrete chunks over time. Schedule plenty of time to move the corresponding jobs to the cloud between data moves. Start your migration by moving low-priority data, such as backups and archives.


A common approach to splitting data for an incremental move is to <store older data in the cloud>, while <keeping your new data in HDFS in your on-premises system>. This enables you to test new and migrated jobs on older, less critical data.

Sometimes you can split your data by looking at the <jobs> that use it. This can be especially helpful if you are migrating jobs incrementally. You can move just the data that is used by the jobs that you move.

In some cases, you can split your data by areas of <ownership>. One advantage of taking this approach is that your data organization aligns with the organization of your business. Another advantage is that it allows you to leverage Google Cloud role-based access management.


One of the challenges of using a hybrid solution is that sometimes a job needs to access data from both Google Cloud and from on-premises systems. If a dataset must be accessed in both environments, you need to establish a primary storage location for it in one environment and maintain a synchronized copy in the other.

The challenges of synchronizing data are similar, regardless of the primary location you choose. You need a way to identify when data has changed and a mechanism to propagate changes to the corresponding copies. If there is a potential for conflicting changes, you also need to develop a strategy for resolving them.

You can use technologies such as Apache Airflow to help manage your data synchronization.

The easiest way to handle access control is to sort data into Cloud Storage buckets based on access requirements and configure your authorization policy at the bucket level. You can assign roles that grant access to individual users or to groups. Grant access by groups, and then assigning users to groups as needed. You need to make data access decisions as you import and structure your data in Cloud Storage.


Because Cloud Storage is a Hadoop compatible file system, you can use Hadoop DistCp to move your data from your on-premises HDFS to Cloud Storage. You can move data several ways using DistCp.


When you're copying or moving data between distinct storage systems such as multiple HDFS clusters or between HDFS and Cloud Storage, it's a good idea to perform some type of validation to guarantee data integrity.