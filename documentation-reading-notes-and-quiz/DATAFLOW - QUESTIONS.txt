Dataflow is a 
	<<______ _________>> No___
	______ analytics 
service, 
that minimizes l______y, p______ time, and c___ through 
	auto______ 
	b______ 
processing.

<H______tal _________> of worker resources for optimum throughput results in better overall price-to-performance.

Flexible resource scheduling pricing for b___ processing, such as _________ jobs

Automated resource management and dynamic work rebalancing

Built on the autoscaling infrastructure of Dataflow along with Pub/Sub and BigQuery, 
	Ingest --> _________ --> Dataflow --> ______

our <streaming solution> provisions the resources you need to ingest, process, and analyze fluctuating volumes of real-time data for real-time business insights. 

<Autoscaling> lets the Dataflow service automatically choose the appropriate number of w______  i_________ required to run your job.


The Apache Beam SDK 

is an open source <<<<_________ ______>>>> for data pipelines.  
can choose a runner, such as ______, to execute your pipeline. 

The simplest pipelines represent a linear flow of operations, as shown in Figure 1 below:

Input -Transform-> _________ -Transform-> PCollection -______-> Output 

A pipeline represents a Directed Acyclic Graph of steps. 

It can have multiple input _________ , 
multiple output _________ , 
and its operations (______) can output multiple _________.

Transforms that use side ______, process each element of the input once, and allow you to <<output>> to zero or more <<additional>>> PCollections.

+ Branching PCollections
+ Multiple transforms process the same PCollection
+ A single transform that uses side outputs
+ Merging PCollections

Often, after you've branched your PCollection into multiple PCollections via multiple transforms, you'll want to merge some or all of those resulting PCollections back together. You can do so by using one of the following:

    ______ - 
	You can use the <______ transform> in the Dataflow SDK to merge multiple PCollections <<of the same type>>.

    ______ - 
	You can use the CoGroupByKey transform in the Dataflow SDK to perform 
	a relational ______ between two PCollections. 

+ Creating Your Pipeline Object
// Start by defining the options for the pipeline.
PipelineOptions options = PipelineOptionsFactory.create();
// Then create the pipeline.
Pipeline p = Pipeline.create(options);

Pipeline Options
    Where your pipeline runs
    Where your pipeline job stages files
    Which Cloud Platform project your pipeline is associated with
    How many Compute Engine instances your pipeline uses as workers

+ Reading Data Into Your Pipeline
PCollection<String> lines = p.apply(
  TextIO.Read.named("ReadMyFile").from("gs://some/inputData.txt"));

There are two kinds of root transforms in the Dataflow Java SDK: ______ and ______ . 
	______ transforms: read data from an ______ source, such as BigQuery or a text file in Google Cloud Storage. 
	______ transforms: create a PCollection from an in-memory java.util.____________.

+ Applying Transforms to Process Pipeline Data

These include general-purpose <core transforms>, such as Par______ or Combine. 

+ Writing or Outputting Your Final Pipeline Data

______ transforms can output the elements of a PCollection to an external data ______, 
such as a file in ____________ or a ____________ table. 

PCollection<String> filteredWords = ...;
filteredWords.apply(TextIO.Write.named("WriteMyFile").to("gs://some/outputData.txt"));

+ Running Your Pipeline
p.run();

Pipelines are executed a____________ly

The monitoring interface can show you:

    A ____________ of all currently running Dataflow jobs and previously run jobs within the last ____________ days.

The execution graph is translated into JSON format, and the JSON execution graph is transmitted to the Dataflow service endpoint.

Dataflow uses the abstractions in the _________ _________ to represent parallel processing functions; for example, your ParDo transforms cause Dataflow to automatically distribute your processing code (represented by DoFns) to multiple workers to be run in parallel.

When running in <batch mode>, bundles including a failing item are retried ____________ times. 
The pipeline will fail completely when a single bundle has failed ____________ times. 

When running in <streaming mode>, a bundle including a failing item will be retried _____________, which may cause your pipeline to permanently stall.

<______ steps> prevents the Dataflow service from needing to <materialize> every intermediate PCollection in your pipeline, which can be costly in terms of memory and processing overhead.

You can disable autoscaling by specifying the flag --__________________=NONE when you run your pipeline; 
if so, note that the Dataflow service sets the number of workers based on the --____________ option, 
which defaults to ______.

You might still cap the number of workers by specifying the --____________ option when you run your pipeline.

Streaming autoscaling is a free feature and is designed to reduce the costs of the resources used when executing streaming pipelines.

Without autoscaling, you choose a fixed number of workers by specifying 
	____________ or 
	____________ 
to execute your pipeline.

You may run up to ____________ concurrent Dataflow jobs per Google Cloud project.

The Dataflow service currently allows a maximum of ____________ Compute Engine instances per job. 
The default machine type is 
	    n1-standard-______ for a batch job, 
	and n1-standard-______ for streaming; 
when using the default machine types, the Dataflow service can therefore allocate up to ____________ cores per job.

The Dataflow service is currently limited to ____________ persistent disks per worker instance when running a streaming job.

the default size of each persistent disk is 
	______ GB in batch mode 
	______ GB in streaming mode.

By default, the Dataflow service deploys Compute Engine resources in the us-central1-f zone of the us-central1 region. 

When you update a job on the Dataflow service, you replace the existing job with a new job that runs your updated pipeline code. 
The Dataflow service retains the job ______, but runs the replacement job with an updated Job ______.

The replacement job preserves any ____________ ______ data from the prior job, as well as any ______ data records or metadata currently "in-flight" from the prior job. 

    Pass the --______ option.
    Set the --______ option in PipelineOptions to the same name as the job you want to update.
    If any transform names in your pipeline have changed, you must supply a transform ______ and pass it using the --transformNameMapping option.

Adding side inputs to or removing them from a transform in your replacement pipeline will cause the compatibility check to fail.

If the replacement job uses different or incompatible data encoding, the Dataflow service will not be able to serialize or deserialize these records.

There are two possible commands you can issue to stop your job: 
	______
	______

Using the ____________ option to stop your job tells the Dataflow service to cancel your job immediately.

"In-flight" data refers to data that has been read but is still being processed by your pipeline. 

______ batch pipelines is not supported.

Your job stops ingesting new data from input sources soon after receiving the ______ request (typically within a few minutes). 
However, the Dataflow service preserves any existing resources, such as worker instances, to finish processing and writing any buffered data in your pipeline. 

+ A ______ VPC network 

is one that exists in a separate project, called a host project, within your organization.

+ VPC ______ Controls

Dataflow VPC ______ Controls provide additional security for your pipeline's resources and services.

A customer-managed encryption key (CMEK) enables encryption of data at rest with a key that you can control through Cloud KMS. 
You can create a batch or streaming pipeline that is protected with a CMEK or access CMEK-protected data in sources and sinks.

+ Create a pipeline that is protected by Cloud KMS

When you create a batch or streaming pipeline, you can select a Cloud KMS key to encrypt the pipeline state. 
The pipeline state is the data that is stored by Dataflow in temporary storage.

+ ParDo
    ParDo is the core ____________ processing operation in the Apache Beam SDKs, invoking a user-specified function on each of the elements of the input PCollection. ParDo collects the zero or more output elements into an output PCollection. The ParDo transform processes elements independently and possibly in parallel.

+ Pipeline I/O
    Apache Beam I/O connectors let you read data into your pipeline and write output data from your pipeline. An I/O connector consists of a ______ and a ______ . All Apache Beam ______ and ______ are transforms that let your pipeline work with data from several different data storage formats. You can also write a custom I/O connector.

+Unbounded PCollections

or unbounded collections, represent data in ______ pipelines. 
An unbounded collection contains data from a ______ ______ data source such as ____________.


+ Windows and windowing functions

Windowing functions divide unbounded collections into logical components, or windows. 
Windowing functions group unbounded collections by the ____________ of the individual elements. 
Each window contains a ____________ number of elements.


+ Tumbling windows

A tumbling window represents a ______, <<<______>>> time interval in the data stream.


+ Hopping windows

A hopping window represents a consistent time interval in the data stream. Hopping windows can ______, whereas tumbling windows are ______.

+ Session windows

A session window contains elements within a ______ duration of another element. The ______ duration is an interval between new data in a data stream. If data arrives after the ______ duration, the data is assigned to a new window.

Dataflow 
compliments Pub/Sub's scalable, at-least-once <delivery> model 
with message ______ and ______, in-order <processing> if you use ______ and ______.